{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4141c148",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# 🤖 Neural Network Algorithms 🧠\n",
    "\n",
    "## 🧾 **Definition (In Simple Words)**\n",
    "\n",
    "**Neural Networks** are algorithms that try to work like a human brain.\n",
    "They learn from data, find patterns, and make smart predictions or decisions.\n",
    "\n",
    "---\n",
    "\n",
    "## 🌟 **Key Things to Know**\n",
    "\n",
    "### 🧱  **Structure**\n",
    "\n",
    "* **Input Layer** 👉 where data enters\n",
    "* **Hidden Layers** 🔄 do the processing\n",
    "* **Output Layer** 🎯 gives the result\n",
    "\n",
    "\n",
    "\n",
    "## 🔄  **Working**\n",
    "\n",
    "* Each connection has a **weight** ⚖️\n",
    "* Data flows through the network\n",
    "* The network adjusts itself using **backpropagation** 🔁 to reduce errors\n",
    "\n",
    "\n",
    "\n",
    "## 📚  **Types of Neural Networks**\n",
    "\n",
    "| Type                          | Use Case               | Emoji |\n",
    "| ----------------------------- | ---------------------- | ----- |\n",
    "| 🤖 **Feedforward NN**         | Basic prediction tasks | 🔍    |\n",
    "| 🔁 **Recurrent NN (RNN)**     | Time series, sequences | ⏳     |\n",
    "| 📸 **Convolutional NN (CNN)** | Image processing       | 🖼️   |\n",
    "| 🧮 **Deep NN (DNN)**          | Complex patterns       | 💡    |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## 🧠  **Important Concepts**\n",
    "\n",
    "* **Activation function** ➡️ Decides whether a neuron should activate 🔓\n",
    "* **Epoch** ➡️ One full pass of training data 🌀\n",
    "* **Loss function** ➡️ Measures prediction error 📉\n",
    "\n",
    "\n",
    "\n",
    "## 🛠️  **Applications**\n",
    "\n",
    "* Image & speech recognition 📷🗣️\n",
    "* Language translation 🌐\n",
    "* Self-driving cars 🚗\n",
    "* Medical diagnosis 🏥\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284f9daf",
   "metadata": {},
   "source": [
    "\n",
    "# ⚡ Activation Function — Simple Explanation 🧾\n",
    "\n",
    "## 🧠 **What is an Activation Function?**\n",
    "\n",
    "- An **activation function** decides whether a neuron should \"fire\" or stay \"silent\" 🔒➡️🔓.\n",
    "- It adds **non-linearity** to the model so that it can learn **complex patterns** 🎯.\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 **Why is it important?**\n",
    "\n",
    "* Helps the network **learn and adapt** 🤓\n",
    "* Allows the model to learn **curved lines**, not just straight ones 📈➡️🔁\n",
    "* Without it, the neural network would be just a **linear model** 😐\n",
    "\n",
    "\n",
    "\n",
    "## 🔥 **Popular Activation Functions**\n",
    "\n",
    "| Function                            | Use                              | Output Range  | Emoji |\n",
    "| ----------------------------------- | -------------------------------- | ------------- | ----- |\n",
    "| 🔘 **Sigmoid**                      | Binary classification            | 0 to 1        | 🌙    |\n",
    "| 🔼 **ReLU (Rectified Linear Unit)** | Most common in deep networks     | 0 to ∞        | 🚀    |\n",
    "| ➿ **Tanh**                          | When outputs need to be centered | -1 to 1       | ⚖️    |\n",
    "| 🌀 **Softmax**                      | Multiclass classification        | Probabilities | 🎯    |\n",
    "\n",
    "\n",
    "\n",
    "## 🔍 **Quick Descriptions**\n",
    "\n",
    "### 🔘 Sigmoid:\n",
    "\n",
    "```math\n",
    "f(x) = 1 / (1 + e^-x)\n",
    "```\n",
    "\n",
    "* S-shaped curve\n",
    "* Good for binary output\n",
    "* 🚫 Can cause vanishing gradient problem\n",
    "\n",
    "\n",
    "\n",
    "### 🔼 ReLU:\n",
    "\n",
    "```math\n",
    "f(x) = max(0, x)\n",
    "```\n",
    "\n",
    "* Super fast & simple\n",
    "* Used in most layers today\n",
    "* ❗ Sometimes dies if neurons get stuck at 0\n",
    "\n",
    "\n",
    "\n",
    "### ➿ Tanh:\n",
    "\n",
    "```math\n",
    "f(x) = (e^x - e^-x) / (e^x + e^-x)\n",
    "```\n",
    "\n",
    "* S-shaped but centered at 0\n",
    "* Better than sigmoid in many cases\n",
    "\n",
    "\n",
    "\n",
    "### 🌀 Softmax:\n",
    "\n",
    "* Used in **final layer** for multiclass classification\n",
    "* Turns scores into **probabilities**\n",
    "* All values add up to 1 🎯\n",
    "\n",
    "\n",
    "\n",
    "## 📌 Summary\n",
    "\n",
    "| Feature            | Sigmoid | ReLU          | Tanh          | Softmax      |\n",
    "| ------------------ | ------- | ------------- | ------------- | ------------ |\n",
    "| Range              | 0–1     | 0–∞           | -1–1          | 0–1          |\n",
    "| Use Case           | Binary  | Hidden layers | Hidden layers | Output layer |\n",
    "| Speed              | Slow    | Fast          | Medium        | Medium       |\n",
    "| Vanishing Gradient | Yes     | No (usually)  | Yes           | No           |\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643e15cb",
   "metadata": {},
   "source": [
    "# ⚙️ **Optimizers & Gradient Techniques** — Simple Notes 🧾✨\n",
    "\n",
    "---\n",
    "\n",
    "## 🔄 **Batch Gradient Descent**\n",
    "\n",
    "* Uses the **entire dataset** to update weights each time\n",
    "\n",
    "* Stable but **very slow** for large datasets 🐢\n",
    "\n",
    "  📌 **Good for small data**\n",
    "\n",
    "  🧠 Example: Academic model training\n",
    "\n",
    "\n",
    "\n",
    "## ⚡ **Stochastic Gradient Descent (SGD)**\n",
    "\n",
    "* Updates weights using **one data point at a time**\n",
    "\n",
    "* **Fast but noisy** updates\n",
    "\n",
    "  📌 **Can bounce around**\n",
    "\n",
    "  🔥 **Helps escape local minima**\n",
    "\n",
    "\n",
    "\n",
    "## ⚖️ **Mini-Batch Gradient Descent**\n",
    "\n",
    "* Mix of **batch** and **stochastic**\n",
    "\n",
    "* Divides data into **small batches** (e.g., 32, 64)\n",
    "\n",
    "  📌 Most **commonly used**\n",
    "\n",
    "  ⚡ **Fast + stable learning**\n",
    "\n",
    "\n",
    "\n",
    "## 🌀 **Momentum-Based Optimizer**\n",
    "\n",
    "* Adds a \"velocity\" term 🏃‍♂️\n",
    "\n",
    "* Remembers the previous update to **smooth out learning**\n",
    "  📉 Reduces oscillations\n",
    "\n",
    "  🚀 **Faster convergence**\n",
    "\n",
    "\n",
    "\n",
    "## 🚀 **Nesterov Accelerated Gradient (NAG)**\n",
    "\n",
    "* Like Momentum but **looks ahead** before updating\n",
    "\n",
    "* Helps prevent **overshooting** 🎯\n",
    "\n",
    "  📌 More accurate and stable\n",
    "\n",
    "  🔍 Think of it like checking before you jump\n",
    "\n",
    "\n",
    "\n",
    "## 📉 **Adagrad**\n",
    "\n",
    "* Adjusts the learning rate for **each parameter**\n",
    "\n",
    "* Parameters with **more updates** get **smaller learning rates**\n",
    "\n",
    "  📌 Great for **sparse data** (e.g., NLP)\n",
    "\n",
    "  ❌ May slow down too much over time\n",
    "\n",
    "\n",
    "\n",
    "## 🔁 **RMSprop**\n",
    "\n",
    "* Improves Adagrad by **controlling the learning rate**\n",
    "\n",
    "* Keeps learning rate stable using a moving average 🧮\n",
    "\n",
    "  📌 Good for **time series / RNNs**\n",
    "\n",
    "  🔄 Keeps training smooth\n",
    "\n",
    "\n",
    "\n",
    "## 🧠 **Adam (Adaptive Moment Estimation)**\n",
    "\n",
    "* Combines **Momentum** + **RMSprop**\n",
    "\n",
    "* Tracks both the average **gradient** and **squared gradient**\n",
    "\n",
    "  📌 Works **great on most tasks**\n",
    "  🚀 Fast, stable, and adaptive = 💯\n",
    "\n",
    "\n",
    "## 🧾 Summary Table\n",
    "\n",
    "| Optimizer / Method   | Key Feature          | Speed             | Use Case         | Emoji |\n",
    "| -------------------- | -------------------- | ----------------- | ---------------- | ----- |\n",
    "| **Batch**            | All data at once     | ❌ Slow            | Small datasets   | 🐢    |\n",
    "| **Stochastic (SGD)** | One sample at a time | ✅ Fast            | Online learning  | ⚡     |\n",
    "| **Mini-Batch**       | Small groups of data | ✅ Balanced        | Most common      | ⚖️    |\n",
    "| **Momentum**         | Adds velocity        | ✅ Fast            | General use      | 🌀    |\n",
    "| **NAG**              | Looks ahead          | ✅ Faster + stable | Sharp minima     | 🚀    |\n",
    "| **Adagrad**          | Per-parameter rate   | ✅ Smart           | Sparse data      | 📉    |\n",
    "| **RMSprop**          | Smoothing + adapt    | ✅ Smart           | RNN, time series | 🔁    |\n",
    "| **Adam**             | Smartest + Fast      | ✅✅ Very Fast      | Deep learning    | 🧠    |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6558bfc5",
   "metadata": {},
   "source": [
    "# 🔺 **Maxima and Minima** 🧾\n",
    "\n",
    "---\n",
    "\n",
    "## 🌟 **What are Maxima and Minima?**\n",
    "\n",
    "* **Maxima** (plural of Maximum) refers to the highest points 🏔️ in a function's curve.\n",
    "* **Minima** (plural of Minimum) refers to the lowest points 🌑 in a function's curve.\n",
    "\n",
    "In simple terms:\n",
    "\n",
    "* **Maxima** = Highest points (Peaks)\n",
    "* **Minima** = Lowest points (Valleys)\n",
    "\n",
    "\n",
    "## 📊 **How do they work in Optimization?**\n",
    "\n",
    "In optimization, we try to find the **maximum or minimum value** of a function, like in neural networks, to minimize **error** or **loss** 📉.\n",
    "\n",
    "* **Minima** → We want to minimize loss (low values = good).\n",
    "* **Maxima** → We want to maximize something (e.g., profit, accuracy).\n",
    "\n",
    "\n",
    "## 🔄 **Types of Maxima and Minima**\n",
    "\n",
    "1. **Local Maxima / Minima**\n",
    "\n",
    "   * These are the highest or lowest points **in a small region** of the graph.\n",
    "   * Not necessarily the **highest or lowest** overall.\n",
    "\n",
    "2. **Global Maxima / Minima**\n",
    "\n",
    "   * These are the **absolute highest** or **lowest** points across the entire function.\n",
    "\n",
    "\n",
    "## 🧾 **Summary**\n",
    "\n",
    "| Term       | Meaning                                 | Example              | Emoji |\n",
    "| ---------- | --------------------------------------- | -------------------- | ----- |\n",
    "| **Maxima** | Highest point (peak)                    | Top of the hill      | 🏔️   |\n",
    "| **Minima** | Lowest point (valley)                   | Bottom of the valley | 🌑    |\n",
    "| **Local**  | High/Low within a region                | Local high in graph  | 🌍    |\n",
    "| **Global** | Absolute highest/lowest in the function | Global highest point | 🌎    |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e265875",
   "metadata": {},
   "source": [
    "## 🔑 **How a Single Neuron Works**\n",
    "\n",
    "1. **Inputs** 🌱: Neuron receives data (e.g., $x_1, x_2$).\n",
    "\n",
    "2. **Weighted Sum** ➕: Multiplies each input by a weight and adds a bias.\n",
    "   Formula:\n",
    "\n",
    "   $$\n",
    "   Z = w_1 \\cdot x_1 + w_2 \\cdot x_2 + b\n",
    "   $$\n",
    "\n",
    "3. **Activation Function** 🔥: Applies a function (e.g., ReLU) to decide if the neuron fires or not.\n",
    "\n",
    "4. **Output** 💡: Final value passed on (or predicted).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c627d4",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 🧠 **Types of Layers in Different Neural Network Models**\n",
    "\n",
    "## **Feedforward Neural Network (FNN)**\n",
    "\n",
    "* **Layers**:\n",
    "\n",
    "  * **Input Layer**: Takes the input features.\n",
    "  * **Hidden Layers**: Multiple layers between input and output for processing.\n",
    "  * **Output Layer**: Produces the final result (e.g., class label, prediction).\n",
    "* **Type of Layer**: Fully connected layers where each neuron connects to every neuron in the next layer.\n",
    "* **Activation**: Typically uses ReLU, Sigmoid, or Tanh.\n",
    "* **Use**: Basic structure for many types of neural networks.\n",
    "\n",
    "\n",
    "\n",
    "## **Recurrent Neural Network (RNN)**\n",
    "\n",
    "* **Layers**:\n",
    "\n",
    "  * **Input Layer**: Takes sequential data (e.g., words, time series).\n",
    "  * **Hidden Layers**: Neurons maintain a **memory** of previous inputs (loops within the network).\n",
    "  * **Output Layer**: Final output is based on current and previous inputs.\n",
    "* **Type of Layer**: **Recurrent layers** (e.g., LSTM, GRU) that can remember past states.\n",
    "* **Activation**: Sigmoid or Tanh (in hidden layers).\n",
    "* **Use**: Sequential data like time series, speech, text.\n",
    "\n",
    "\n",
    "\n",
    "## **Convolutional Neural Network (CNN)**\n",
    "\n",
    "* **Layers**:\n",
    "\n",
    "  * **Input Layer**: Takes image or grid-like data.\n",
    "  * **Convolutional Layers**: Filters to detect patterns (edges, textures, etc.) in data.\n",
    "  * **Pooling Layers**: Reduces dimensions and computational complexity (e.g., Max Pooling).\n",
    "  * **Fully Connected Layers**: After convolution and pooling, fully connected layers for final predictions.\n",
    "* **Type of Layer**: **Convolutional** and **Pooling layers** are key.\n",
    "* **Activation**: ReLU (most common for convolution layers), Softmax for output.\n",
    "* **Use**: Image classification, object detection, and similar tasks.\n",
    "\n",
    "\n",
    "\n",
    "## **Deep Neural Network (DNN)**\n",
    "\n",
    "* **Layers**:\n",
    "\n",
    "  * **Input Layer**: Accepts the input features.\n",
    "  * **Hidden Layers**: Multiple layers, usually with many neurons, to extract complex patterns.\n",
    "  * **Output Layer**: Produces the prediction or classification.\n",
    "* **Type of Layer**: **Fully connected layers** with deep architectures.\n",
    "* **Activation**: ReLU for hidden layers, Softmax for classification output.\n",
    "* **Use**: Complex tasks where deep architectures are needed for higher accuracy.\n",
    "\n",
    "\n",
    "\n",
    "## ✨ **Summary**\n",
    "\n",
    "| Model   | Key Layers                              | Main Use           | Emoji |\n",
    "| ------- | --------------------------------------- | ------------------ | ----- |\n",
    "| **FNN** | Input, Hidden, Output                   | General-purpose NN | 🧠    |\n",
    "| **RNN** | Input, Hidden (Recurrent), Output       | Sequential data    | 🔄    |\n",
    "| **CNN** | Convolutional, Pooling, Fully Connected | Image recognition  | 🖼️   |\n",
    "| **DNN** | Input, Hidden (Deep), Output            | Complex tasks      | 🌌    |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9694c1ef",
   "metadata": {},
   "source": [
    "\n",
    "# 🔍 **Perceptron**\n",
    "\n",
    "- The **Perceptron** is one of the simplest types of neural networks, inspired by the way biological neurons work. It's used for binary classification tasks.\n",
    "- The **Perceptron** is the foundation of more complex neural networks, with its simple learning rule and binary output. It's great for problems like **AND/OR classification**!\n",
    "\n",
    "\n",
    "## 🌱 **How It Works:**\n",
    "\n",
    "1. **Inputs**: Takes inputs (data features) like numbers.\n",
    "2. **Weights**: Each input has an associated weight (importance of each feature).\n",
    "3. **Bias**: A bias is added to adjust the output independently of the inputs.\n",
    "4. **Summation**: The inputs are multiplied by their respective weights, summed up, and then the bias is added.\n",
    "5. **Activation Function**: The sum is passed through an **activation function** (often **Step function**).\n",
    "\n",
    "   * If the output is above a threshold, the neuron \"fires\" (outputs 1).\n",
    "   * If it’s below, it doesn’t fire (outputs 0).\n",
    "\n",
    "\n",
    "\n",
    "## 🔑 **Key Components**:\n",
    "\n",
    "1. **Weights (w)**: Adjusted during training to minimize error.\n",
    "2. **Bias (b)**: Helps the model make decisions even when all inputs are zero.\n",
    "3. **Activation Function**: Typically a **Step function** for binary classification.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## 🚀 **Summary of Perceptron:**\n",
    "\n",
    "| Feature        | Description                           | Emoji |\n",
    "| -------------- | ------------------------------------- | ----- |\n",
    "| **Input**      | Features of data                      | 🌱    |\n",
    "| **Weights**    | Importance of each feature            | ⚖️    |\n",
    "| **Bias**       | Adjusts output                        | ✨     |\n",
    "| **Activation** | Determines if neuron fires (0 or 1)   | 🔥    |\n",
    "| **Output**     | Binary classification result (0 or 1) | ✅❌    |\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9445ea8e",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# 🔄 **Backpropagation** — Learning Process in Neural Networks\n",
    "\n",
    "- **Backpropagation** is the key algorithm used for training neural networks. \n",
    "- It helps adjust the weights of the network based on the error in the predictions.\n",
    "- Backpropagation allows the neural network to **learn** from its mistakes by adjusting its weights after each training iteration!\n",
    "\n",
    "---\n",
    "\n",
    "## 🌱 **How Backpropagation Works**:\n",
    "\n",
    "1. **Forward Pass**:\n",
    "\n",
    "   * The input data is passed through the network to get an output.\n",
    "\n",
    "2. **Calculate Error**:\n",
    "\n",
    "   * Compare the predicted output to the actual target value using a **loss function** (e.g., Mean Squared Error).\n",
    "\n",
    "3. **Backward Pass**:\n",
    "\n",
    "   * The error is **propagated backward** from the output layer to the input layer, layer by layer.\n",
    "   * The goal is to update the weights so that the error is minimized in future predictions.\n",
    "\n",
    "4. **Gradient Descent**:\n",
    "\n",
    "   * During the backward pass, the **gradient of the error** is calculated for each weight.\n",
    "   * The weights are updated using **gradient descent** to minimize the error:\n",
    "\n",
    "     $$\n",
    "     w_{\\text{new}} = w_{\\text{old}} - \\eta \\cdot \\frac{\\partial E}{\\partial w}\n",
    "     $$\n",
    "\n",
    "     Where:\n",
    "\n",
    "     * $\\eta$ is the **learning rate**\n",
    "     * $\\frac{\\partial E}{\\partial w}$ is the **gradient** of the error with respect to the weight.\n",
    "\n",
    "\n",
    "\n",
    "## 🚀 **Summary of Backpropagation:**\n",
    "\n",
    "| Step                 | Description                                    | Emoji |\n",
    "| -------------------- | ---------------------------------------------- | ----- |\n",
    "| **Forward Pass**     | Compute output using current weights           | 🏃‍♂️ |\n",
    "| **Calculate Error**  | Find the difference between predicted and true | ❌     |\n",
    "| **Backward Pass**    | Propagate the error to adjust weights          | 🔄    |\n",
    "| **Gradient Descent** | Update weights to reduce error                 | 🧑‍🏫 |\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b586a75",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 🧑‍🏫 **Learning Rate** — What is it?\n",
    "\n",
    "- The **learning rate** is a hyperparameter that controls how much the weights of the neural network are adjusted during training.\n",
    "- It helps determine the **step size** taken towards minimizing the error.\n",
    "- The **learning rate** is a key factor in determining how efficiently a neural network learns. You want it to be **just right** to speed up the process while ensuring stability! 😊\n",
    "---\n",
    "\n",
    "## 🌱 **How It Works:**\n",
    "\n",
    "1. **Too Small Learning Rate**:\n",
    "\n",
    "   * **Slow Training** 🐢\n",
    "   * The model takes tiny steps, and it may take a very long time to converge to the optimal solution.\n",
    "\n",
    "2. **Too Large Learning Rate**:\n",
    "\n",
    "   * **Risk of Overshooting** 🚀\n",
    "   * The model may skip over the optimal solution and never converge, resulting in instability.\n",
    "\n",
    "3. **Ideal Learning Rate**:\n",
    "\n",
    "   * **Balance** ⚖️\n",
    "   * A good learning rate helps the model converge quickly without overshooting.\n",
    "\n",
    "\n",
    "\n",
    "## 🧑‍🏫 **How Learning Rate Affects Gradient Descent**:\n",
    "\n",
    "During training, the weights are updated using **gradient descent**:\n",
    "\n",
    "$$\n",
    "w_{\\text{new}} = w_{\\text{old}} - \\eta \\cdot \\frac{\\partial E}{\\partial w}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "* $\\eta$ is the **learning rate**\n",
    "* $\\frac{\\partial E}{\\partial w}$ is the **gradient of the error** with respect to the weight.\n",
    "\n",
    "---\n",
    "\n",
    "## 🚀 **Summary of Learning Rate**:\n",
    "\n",
    "| Learning Rate  | Effect                               | Emoji |\n",
    "| -------------- | ------------------------------------ | ----- |\n",
    "| **Too Small**  | Slow convergence, takes long time    | 🐢    |\n",
    "| **Too Large**  | Risk of overshooting and instability | 🚀    |\n",
    "| **Ideal Rate** | Balanced, optimal learning pace      | ⚖️    |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1149e360",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
