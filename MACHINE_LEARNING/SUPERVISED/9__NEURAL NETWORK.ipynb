{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4141c148",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# ğŸ¤– Neural Network Algorithms ğŸ§ \n",
    "\n",
    "## ğŸ§¾ **Definition (In Simple Words)**\n",
    "\n",
    "**Neural Networks** are algorithms that try to work like a human brain.\n",
    "They learn from data, find patterns, and make smart predictions or decisions.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸŒŸ **Key Things to Know**\n",
    "\n",
    "### ğŸ§±  **Structure**\n",
    "\n",
    "* **Input Layer** ğŸ‘‰ where data enters\n",
    "* **Hidden Layers** ğŸ”„ do the processing\n",
    "* **Output Layer** ğŸ¯ gives the result\n",
    "\n",
    "\n",
    "\n",
    "## ğŸ”„  **Working**\n",
    "\n",
    "* Each connection has a **weight** âš–ï¸\n",
    "* Data flows through the network\n",
    "* The network adjusts itself using **backpropagation** ğŸ” to reduce errors\n",
    "\n",
    "\n",
    "\n",
    "## ğŸ“š  **Types of Neural Networks**\n",
    "\n",
    "| Type                          | Use Case               | Emoji |\n",
    "| ----------------------------- | ---------------------- | ----- |\n",
    "| ğŸ¤– **Feedforward NN**         | Basic prediction tasks | ğŸ”    |\n",
    "| ğŸ” **Recurrent NN (RNN)**     | Time series, sequences | â³     |\n",
    "| ğŸ“¸ **Convolutional NN (CNN)** | Image processing       | ğŸ–¼ï¸   |\n",
    "| ğŸ§® **Deep NN (DNN)**          | Complex patterns       | ğŸ’¡    |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## ğŸ§   **Important Concepts**\n",
    "\n",
    "* **Activation function** â¡ï¸ Decides whether a neuron should activate ğŸ”“\n",
    "* **Epoch** â¡ï¸ One full pass of training data ğŸŒ€\n",
    "* **Loss function** â¡ï¸ Measures prediction error ğŸ“‰\n",
    "\n",
    "\n",
    "\n",
    "## ğŸ› ï¸  **Applications**\n",
    "\n",
    "* Image & speech recognition ğŸ“·ğŸ—£ï¸\n",
    "* Language translation ğŸŒ\n",
    "* Self-driving cars ğŸš—\n",
    "* Medical diagnosis ğŸ¥\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284f9daf",
   "metadata": {},
   "source": [
    "\n",
    "# âš¡ Activation Function â€” Simple Explanation ğŸ§¾\n",
    "\n",
    "## ğŸ§  **What is an Activation Function?**\n",
    "\n",
    "- An **activation function** decides whether a neuron should \"fire\" or stay \"silent\" ğŸ”’â¡ï¸ğŸ”“.\n",
    "- It adds **non-linearity** to the model so that it can learn **complex patterns** ğŸ¯.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ **Why is it important?**\n",
    "\n",
    "* Helps the network **learn and adapt** ğŸ¤“\n",
    "* Allows the model to learn **curved lines**, not just straight ones ğŸ“ˆâ¡ï¸ğŸ”\n",
    "* Without it, the neural network would be just a **linear model** ğŸ˜\n",
    "\n",
    "\n",
    "\n",
    "## ğŸ”¥ **Popular Activation Functions**\n",
    "\n",
    "| Function                            | Use                              | Output Range  | Emoji |\n",
    "| ----------------------------------- | -------------------------------- | ------------- | ----- |\n",
    "| ğŸ”˜ **Sigmoid**                      | Binary classification            | 0 to 1        | ğŸŒ™    |\n",
    "| ğŸ”¼ **ReLU (Rectified Linear Unit)** | Most common in deep networks     | 0 to âˆ        | ğŸš€    |\n",
    "| â¿ **Tanh**                          | When outputs need to be centered | -1 to 1       | âš–ï¸    |\n",
    "| ğŸŒ€ **Softmax**                      | Multiclass classification        | Probabilities | ğŸ¯    |\n",
    "\n",
    "\n",
    "\n",
    "## ğŸ” **Quick Descriptions**\n",
    "\n",
    "### ğŸ”˜ Sigmoid:\n",
    "\n",
    "```math\n",
    "f(x) = 1 / (1 + e^-x)\n",
    "```\n",
    "\n",
    "* S-shaped curve\n",
    "* Good for binary output\n",
    "* ğŸš« Can cause vanishing gradient problem\n",
    "\n",
    "\n",
    "\n",
    "### ğŸ”¼ ReLU:\n",
    "\n",
    "```math\n",
    "f(x) = max(0, x)\n",
    "```\n",
    "\n",
    "* Super fast & simple\n",
    "* Used in most layers today\n",
    "* â— Sometimes dies if neurons get stuck at 0\n",
    "\n",
    "\n",
    "\n",
    "### â¿ Tanh:\n",
    "\n",
    "```math\n",
    "f(x) = (e^x - e^-x) / (e^x + e^-x)\n",
    "```\n",
    "\n",
    "* S-shaped but centered at 0\n",
    "* Better than sigmoid in many cases\n",
    "\n",
    "\n",
    "\n",
    "### ğŸŒ€ Softmax:\n",
    "\n",
    "* Used in **final layer** for multiclass classification\n",
    "* Turns scores into **probabilities**\n",
    "* All values add up to 1 ğŸ¯\n",
    "\n",
    "\n",
    "\n",
    "## ğŸ“Œ Summary\n",
    "\n",
    "| Feature            | Sigmoid | ReLU          | Tanh          | Softmax      |\n",
    "| ------------------ | ------- | ------------- | ------------- | ------------ |\n",
    "| Range              | 0â€“1     | 0â€“âˆ           | -1â€“1          | 0â€“1          |\n",
    "| Use Case           | Binary  | Hidden layers | Hidden layers | Output layer |\n",
    "| Speed              | Slow    | Fast          | Medium        | Medium       |\n",
    "| Vanishing Gradient | Yes     | No (usually)  | Yes           | No           |\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643e15cb",
   "metadata": {},
   "source": [
    "# âš™ï¸ **Optimizers & Gradient Techniques** â€” Simple Notes ğŸ§¾âœ¨\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”„ **Batch Gradient Descent**\n",
    "\n",
    "* Uses the **entire dataset** to update weights each time\n",
    "\n",
    "* Stable but **very slow** for large datasets ğŸ¢\n",
    "\n",
    "  ğŸ“Œ **Good for small data**\n",
    "\n",
    "  ğŸ§  Example: Academic model training\n",
    "\n",
    "\n",
    "\n",
    "## âš¡ **Stochastic Gradient Descent (SGD)**\n",
    "\n",
    "* Updates weights using **one data point at a time**\n",
    "\n",
    "* **Fast but noisy** updates\n",
    "\n",
    "  ğŸ“Œ **Can bounce around**\n",
    "\n",
    "  ğŸ”¥ **Helps escape local minima**\n",
    "\n",
    "\n",
    "\n",
    "## âš–ï¸ **Mini-Batch Gradient Descent**\n",
    "\n",
    "* Mix of **batch** and **stochastic**\n",
    "\n",
    "* Divides data into **small batches** (e.g., 32, 64)\n",
    "\n",
    "  ğŸ“Œ Most **commonly used**\n",
    "\n",
    "  âš¡ **Fast + stable learning**\n",
    "\n",
    "\n",
    "\n",
    "## ğŸŒ€ **Momentum-Based Optimizer**\n",
    "\n",
    "* Adds a \"velocity\" term ğŸƒâ€â™‚ï¸\n",
    "\n",
    "* Remembers the previous update to **smooth out learning**\n",
    "  ğŸ“‰ Reduces oscillations\n",
    "\n",
    "  ğŸš€ **Faster convergence**\n",
    "\n",
    "\n",
    "\n",
    "## ğŸš€ **Nesterov Accelerated Gradient (NAG)**\n",
    "\n",
    "* Like Momentum but **looks ahead** before updating\n",
    "\n",
    "* Helps prevent **overshooting** ğŸ¯\n",
    "\n",
    "  ğŸ“Œ More accurate and stable\n",
    "\n",
    "  ğŸ” Think of it like checking before you jump\n",
    "\n",
    "\n",
    "\n",
    "## ğŸ“‰ **Adagrad**\n",
    "\n",
    "* Adjusts the learning rate for **each parameter**\n",
    "\n",
    "* Parameters with **more updates** get **smaller learning rates**\n",
    "\n",
    "  ğŸ“Œ Great for **sparse data** (e.g., NLP)\n",
    "\n",
    "  âŒ May slow down too much over time\n",
    "\n",
    "\n",
    "\n",
    "## ğŸ” **RMSprop**\n",
    "\n",
    "* Improves Adagrad by **controlling the learning rate**\n",
    "\n",
    "* Keeps learning rate stable using a moving average ğŸ§®\n",
    "\n",
    "  ğŸ“Œ Good for **time series / RNNs**\n",
    "\n",
    "  ğŸ”„ Keeps training smooth\n",
    "\n",
    "\n",
    "\n",
    "## ğŸ§  **Adam (Adaptive Moment Estimation)**\n",
    "\n",
    "* Combines **Momentum** + **RMSprop**\n",
    "\n",
    "* Tracks both the average **gradient** and **squared gradient**\n",
    "\n",
    "  ğŸ“Œ Works **great on most tasks**\n",
    "  ğŸš€ Fast, stable, and adaptive = ğŸ’¯\n",
    "\n",
    "\n",
    "## ğŸ§¾ Summary Table\n",
    "\n",
    "| Optimizer / Method   | Key Feature          | Speed             | Use Case         | Emoji |\n",
    "| -------------------- | -------------------- | ----------------- | ---------------- | ----- |\n",
    "| **Batch**            | All data at once     | âŒ Slow            | Small datasets   | ğŸ¢    |\n",
    "| **Stochastic (SGD)** | One sample at a time | âœ… Fast            | Online learning  | âš¡     |\n",
    "| **Mini-Batch**       | Small groups of data | âœ… Balanced        | Most common      | âš–ï¸    |\n",
    "| **Momentum**         | Adds velocity        | âœ… Fast            | General use      | ğŸŒ€    |\n",
    "| **NAG**              | Looks ahead          | âœ… Faster + stable | Sharp minima     | ğŸš€    |\n",
    "| **Adagrad**          | Per-parameter rate   | âœ… Smart           | Sparse data      | ğŸ“‰    |\n",
    "| **RMSprop**          | Smoothing + adapt    | âœ… Smart           | RNN, time series | ğŸ”    |\n",
    "| **Adam**             | Smartest + Fast      | âœ…âœ… Very Fast      | Deep learning    | ğŸ§     |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6558bfc5",
   "metadata": {},
   "source": [
    "# ğŸ”º **Maxima and Minima** ğŸ§¾\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸŒŸ **What are Maxima and Minima?**\n",
    "\n",
    "* **Maxima** (plural of Maximum) refers to the highest points ğŸ”ï¸ in a function's curve.\n",
    "* **Minima** (plural of Minimum) refers to the lowest points ğŸŒ‘ in a function's curve.\n",
    "\n",
    "In simple terms:\n",
    "\n",
    "* **Maxima** = Highest points (Peaks)\n",
    "* **Minima** = Lowest points (Valleys)\n",
    "\n",
    "\n",
    "## ğŸ“Š **How do they work in Optimization?**\n",
    "\n",
    "In optimization, we try to find the **maximum or minimum value** of a function, like in neural networks, to minimize **error** or **loss** ğŸ“‰.\n",
    "\n",
    "* **Minima** â†’ We want to minimize loss (low values = good).\n",
    "* **Maxima** â†’ We want to maximize something (e.g., profit, accuracy).\n",
    "\n",
    "\n",
    "## ğŸ”„ **Types of Maxima and Minima**\n",
    "\n",
    "1. **Local Maxima / Minima**\n",
    "\n",
    "   * These are the highest or lowest points **in a small region** of the graph.\n",
    "   * Not necessarily the **highest or lowest** overall.\n",
    "\n",
    "2. **Global Maxima / Minima**\n",
    "\n",
    "   * These are the **absolute highest** or **lowest** points across the entire function.\n",
    "\n",
    "\n",
    "## ğŸ§¾ **Summary**\n",
    "\n",
    "| Term       | Meaning                                 | Example              | Emoji |\n",
    "| ---------- | --------------------------------------- | -------------------- | ----- |\n",
    "| **Maxima** | Highest point (peak)                    | Top of the hill      | ğŸ”ï¸   |\n",
    "| **Minima** | Lowest point (valley)                   | Bottom of the valley | ğŸŒ‘    |\n",
    "| **Local**  | High/Low within a region                | Local high in graph  | ğŸŒ    |\n",
    "| **Global** | Absolute highest/lowest in the function | Global highest point | ğŸŒ    |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e265875",
   "metadata": {},
   "source": [
    "## ğŸ”‘ **How a Single Neuron Works**\n",
    "\n",
    "1. **Inputs** ğŸŒ±: Neuron receives data (e.g., $x_1, x_2$).\n",
    "\n",
    "2. **Weighted Sum** â•: Multiplies each input by a weight and adds a bias.\n",
    "   Formula:\n",
    "\n",
    "   $$\n",
    "   Z = w_1 \\cdot x_1 + w_2 \\cdot x_2 + b\n",
    "   $$\n",
    "\n",
    "3. **Activation Function** ğŸ”¥: Applies a function (e.g., ReLU) to decide if the neuron fires or not.\n",
    "\n",
    "4. **Output** ğŸ’¡: Final value passed on (or predicted).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c627d4",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## ğŸ§  **Types of Layers in Different Neural Network Models**\n",
    "\n",
    "## **Feedforward Neural Network (FNN)**\n",
    "\n",
    "* **Layers**:\n",
    "\n",
    "  * **Input Layer**: Takes the input features.\n",
    "  * **Hidden Layers**: Multiple layers between input and output for processing.\n",
    "  * **Output Layer**: Produces the final result (e.g., class label, prediction).\n",
    "* **Type of Layer**: Fully connected layers where each neuron connects to every neuron in the next layer.\n",
    "* **Activation**: Typically uses ReLU, Sigmoid, or Tanh.\n",
    "* **Use**: Basic structure for many types of neural networks.\n",
    "\n",
    "\n",
    "\n",
    "## **Recurrent Neural Network (RNN)**\n",
    "\n",
    "* **Layers**:\n",
    "\n",
    "  * **Input Layer**: Takes sequential data (e.g., words, time series).\n",
    "  * **Hidden Layers**: Neurons maintain a **memory** of previous inputs (loops within the network).\n",
    "  * **Output Layer**: Final output is based on current and previous inputs.\n",
    "* **Type of Layer**: **Recurrent layers** (e.g., LSTM, GRU) that can remember past states.\n",
    "* **Activation**: Sigmoid or Tanh (in hidden layers).\n",
    "* **Use**: Sequential data like time series, speech, text.\n",
    "\n",
    "\n",
    "\n",
    "## **Convolutional Neural Network (CNN)**\n",
    "\n",
    "* **Layers**:\n",
    "\n",
    "  * **Input Layer**: Takes image or grid-like data.\n",
    "  * **Convolutional Layers**: Filters to detect patterns (edges, textures, etc.) in data.\n",
    "  * **Pooling Layers**: Reduces dimensions and computational complexity (e.g., Max Pooling).\n",
    "  * **Fully Connected Layers**: After convolution and pooling, fully connected layers for final predictions.\n",
    "* **Type of Layer**: **Convolutional** and **Pooling layers** are key.\n",
    "* **Activation**: ReLU (most common for convolution layers), Softmax for output.\n",
    "* **Use**: Image classification, object detection, and similar tasks.\n",
    "\n",
    "\n",
    "\n",
    "## **Deep Neural Network (DNN)**\n",
    "\n",
    "* **Layers**:\n",
    "\n",
    "  * **Input Layer**: Accepts the input features.\n",
    "  * **Hidden Layers**: Multiple layers, usually with many neurons, to extract complex patterns.\n",
    "  * **Output Layer**: Produces the prediction or classification.\n",
    "* **Type of Layer**: **Fully connected layers** with deep architectures.\n",
    "* **Activation**: ReLU for hidden layers, Softmax for classification output.\n",
    "* **Use**: Complex tasks where deep architectures are needed for higher accuracy.\n",
    "\n",
    "\n",
    "\n",
    "## âœ¨ **Summary**\n",
    "\n",
    "| Model   | Key Layers                              | Main Use           | Emoji |\n",
    "| ------- | --------------------------------------- | ------------------ | ----- |\n",
    "| **FNN** | Input, Hidden, Output                   | General-purpose NN | ğŸ§     |\n",
    "| **RNN** | Input, Hidden (Recurrent), Output       | Sequential data    | ğŸ”„    |\n",
    "| **CNN** | Convolutional, Pooling, Fully Connected | Image recognition  | ğŸ–¼ï¸   |\n",
    "| **DNN** | Input, Hidden (Deep), Output            | Complex tasks      | ğŸŒŒ    |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9694c1ef",
   "metadata": {},
   "source": [
    "\n",
    "# ğŸ” **Perceptron**\n",
    "\n",
    "- The **Perceptron** is one of the simplest types of neural networks, inspired by the way biological neurons work. It's used for binary classification tasks.\n",
    "- The **Perceptron** is the foundation of more complex neural networks, with its simple learning rule and binary output. It's great for problems like **AND/OR classification**!\n",
    "\n",
    "\n",
    "## ğŸŒ± **How It Works:**\n",
    "\n",
    "1. **Inputs**: Takes inputs (data features) like numbers.\n",
    "2. **Weights**: Each input has an associated weight (importance of each feature).\n",
    "3. **Bias**: A bias is added to adjust the output independently of the inputs.\n",
    "4. **Summation**: The inputs are multiplied by their respective weights, summed up, and then the bias is added.\n",
    "5. **Activation Function**: The sum is passed through an **activation function** (often **Step function**).\n",
    "\n",
    "   * If the output is above a threshold, the neuron \"fires\" (outputs 1).\n",
    "   * If itâ€™s below, it doesnâ€™t fire (outputs 0).\n",
    "\n",
    "\n",
    "\n",
    "## ğŸ”‘ **Key Components**:\n",
    "\n",
    "1. **Weights (w)**: Adjusted during training to minimize error.\n",
    "2. **Bias (b)**: Helps the model make decisions even when all inputs are zero.\n",
    "3. **Activation Function**: Typically a **Step function** for binary classification.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## ğŸš€ **Summary of Perceptron:**\n",
    "\n",
    "| Feature        | Description                           | Emoji |\n",
    "| -------------- | ------------------------------------- | ----- |\n",
    "| **Input**      | Features of data                      | ğŸŒ±    |\n",
    "| **Weights**    | Importance of each feature            | âš–ï¸    |\n",
    "| **Bias**       | Adjusts output                        | âœ¨     |\n",
    "| **Activation** | Determines if neuron fires (0 or 1)   | ğŸ”¥    |\n",
    "| **Output**     | Binary classification result (0 or 1) | âœ…âŒ    |\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9445ea8e",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# ğŸ”„ **Backpropagation** â€” Learning Process in Neural Networks\n",
    "\n",
    "- **Backpropagation** is the key algorithm used for training neural networks. \n",
    "- It helps adjust the weights of the network based on the error in the predictions.\n",
    "- Backpropagation allows the neural network to **learn** from its mistakes by adjusting its weights after each training iteration!\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸŒ± **How Backpropagation Works**:\n",
    "\n",
    "1. **Forward Pass**:\n",
    "\n",
    "   * The input data is passed through the network to get an output.\n",
    "\n",
    "2. **Calculate Error**:\n",
    "\n",
    "   * Compare the predicted output to the actual target value using a **loss function** (e.g., Mean Squared Error).\n",
    "\n",
    "3. **Backward Pass**:\n",
    "\n",
    "   * The error is **propagated backward** from the output layer to the input layer, layer by layer.\n",
    "   * The goal is to update the weights so that the error is minimized in future predictions.\n",
    "\n",
    "4. **Gradient Descent**:\n",
    "\n",
    "   * During the backward pass, the **gradient of the error** is calculated for each weight.\n",
    "   * The weights are updated using **gradient descent** to minimize the error:\n",
    "\n",
    "     $$\n",
    "     w_{\\text{new}} = w_{\\text{old}} - \\eta \\cdot \\frac{\\partial E}{\\partial w}\n",
    "     $$\n",
    "\n",
    "     Where:\n",
    "\n",
    "     * $\\eta$ is the **learning rate**\n",
    "     * $\\frac{\\partial E}{\\partial w}$ is the **gradient** of the error with respect to the weight.\n",
    "\n",
    "\n",
    "\n",
    "## ğŸš€ **Summary of Backpropagation:**\n",
    "\n",
    "| Step                 | Description                                    | Emoji |\n",
    "| -------------------- | ---------------------------------------------- | ----- |\n",
    "| **Forward Pass**     | Compute output using current weights           | ğŸƒâ€â™‚ï¸ |\n",
    "| **Calculate Error**  | Find the difference between predicted and true | âŒ     |\n",
    "| **Backward Pass**    | Propagate the error to adjust weights          | ğŸ”„    |\n",
    "| **Gradient Descent** | Update weights to reduce error                 | ğŸ§‘â€ğŸ« |\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b586a75",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## ğŸ§‘â€ğŸ« **Learning Rate** â€” What is it?\n",
    "\n",
    "- The **learning rate** is a hyperparameter that controls how much the weights of the neural network are adjusted during training.\n",
    "- It helps determine the **step size** taken towards minimizing the error.\n",
    "- The **learning rate** is a key factor in determining how efficiently a neural network learns. You want it to be **just right** to speed up the process while ensuring stability! ğŸ˜Š\n",
    "---\n",
    "\n",
    "## ğŸŒ± **How It Works:**\n",
    "\n",
    "1. **Too Small Learning Rate**:\n",
    "\n",
    "   * **Slow Training** ğŸ¢\n",
    "   * The model takes tiny steps, and it may take a very long time to converge to the optimal solution.\n",
    "\n",
    "2. **Too Large Learning Rate**:\n",
    "\n",
    "   * **Risk of Overshooting** ğŸš€\n",
    "   * The model may skip over the optimal solution and never converge, resulting in instability.\n",
    "\n",
    "3. **Ideal Learning Rate**:\n",
    "\n",
    "   * **Balance** âš–ï¸\n",
    "   * A good learning rate helps the model converge quickly without overshooting.\n",
    "\n",
    "\n",
    "\n",
    "## ğŸ§‘â€ğŸ« **How Learning Rate Affects Gradient Descent**:\n",
    "\n",
    "During training, the weights are updated using **gradient descent**:\n",
    "\n",
    "$$\n",
    "w_{\\text{new}} = w_{\\text{old}} - \\eta \\cdot \\frac{\\partial E}{\\partial w}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "* $\\eta$ is the **learning rate**\n",
    "* $\\frac{\\partial E}{\\partial w}$ is the **gradient of the error** with respect to the weight.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸš€ **Summary of Learning Rate**:\n",
    "\n",
    "| Learning Rate  | Effect                               | Emoji |\n",
    "| -------------- | ------------------------------------ | ----- |\n",
    "| **Too Small**  | Slow convergence, takes long time    | ğŸ¢    |\n",
    "| **Too Large**  | Risk of overshooting and instability | ğŸš€    |\n",
    "| **Ideal Rate** | Balanced, optimal learning pace      | âš–ï¸    |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1149e360",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
