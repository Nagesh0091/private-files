{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6dd32a78",
   "metadata": {},
   "source": [
    "### ğŸ¯ **Logistic Regression ğŸ¯**\n",
    "\n",
    "### ğŸ“Œ **Definition:**\n",
    "\n",
    "- Logistic Regression is used to predict a binary or categorical outcome (1 or 0, true or false) based on one or more input features.\n",
    "- Goal is to predict the probability of an observation belonging one of two categories.\n",
    "### ğŸ“ˆ **Equation:**\n",
    "\n",
    "**P(Y = 1) = 1 / (1 + e^-(Î²â‚€ + Î²â‚Xâ‚ + Î²â‚‚Xâ‚‚ + ... + Î²â‚™Xâ‚™))**\n",
    "\n",
    "â†’ P(Y = 1) = Probability of the target being 1\n",
    "\n",
    "â†’ X = Features\n",
    "\n",
    "â†’ Î² = Coefficients\n",
    "\n",
    "â†’ e = Eulerâ€™s number (approx. 2.718)\n",
    "\n",
    "### ğŸ§  **Key Assumptions (VERY IMPORTANT!)**\n",
    "\n",
    "âœ… **Linearity in the Logit** â€“ Relationship between features and the log-odds of the target is linear.\n",
    "\n",
    "âœ… **Independence of observations** â€“ Each observation is independent.\n",
    "\n",
    "âœ… **No multicollinearity** â€“ Predictors should not be highly correlated.\n",
    "\n",
    "âœ… **Large Sample Size** â€“ Logistic regression generally requires a larger sample size for reliable results.\n",
    "\n",
    "### ğŸ§® **Types:**\n",
    "\n",
    "* **Binary Logistic Regression** â†’ One dependent binary variable (e.g., yes/no, 0/1).\n",
    "* **Multinomial Logistic Regression** â†’ More than two categories for the dependent variable.\n",
    "\n",
    "### ğŸ“Š **Evaluation Metrics:**\n",
    "\n",
    "* **Accuracy** â€“ Percentage of correct predictions.\n",
    "* **Precision, Recall, F1-Score** â€“ Metrics for imbalanced classes.\n",
    "* **AUC-ROC Curve** â€“ Evaluates classification performance at all thresholds.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4997ed87",
   "metadata": {},
   "source": [
    "Sure! Here's the updated version with your point added:\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… **Logistic Function (Sigmoid Function)**\n",
    "\n",
    "The **logistic function**, also known as the **sigmoid function**, is a mathematical function used to model the **probability of a binary outcome**. It produces an **S-shaped curve** that maps any real-valued number to a value between **0 and 1**.\n",
    "\n",
    "### ğŸ“ˆ **Formula:**\n",
    "\n",
    "$$\n",
    "f(x) = \\frac{1}{1 + e^{-x}}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "* **x** = Linear combination (Î²â‚€ + Î²â‚Xâ‚ + Î²â‚‚Xâ‚‚ + ... + Î²â‚™Xâ‚™)\n",
    "* **e** = Eulerâ€™s number (\\~2.718)\n",
    "\n",
    "### ğŸ” **Output Behavior:**\n",
    "\n",
    "* **x â†’ +âˆ** â†’ **f(x) â†’ 1**\n",
    "* **x â†’ -âˆ** â†’ **f(x) â†’ 0**\n",
    "* **x = 0** â†’ **f(x) = 0.5**\n",
    "\n",
    "### ğŸ“Š **Use in Logistic Regression:**\n",
    "\n",
    "* Converts the linear output into a **probability**.\n",
    "* Used to classify:\n",
    "\n",
    "  * **f(x) â‰¥ 0.5** â†’ Class 1\n",
    "  * **f(x) < 0.5** â†’ Class 0\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a4d7d33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0000\n",
      "Precision: 1.0000\n",
      "Recall: 1.0000\n",
      "F1 Score: 1.0000\n",
      "Confusion Matrix:\n",
      "[[1 0]\n",
      " [0 2]]\n"
     ]
    }
   ],
   "source": [
    "# LOGISTIC REGRESSION:\n",
    "DATA = {\n",
    "    'Age': [22, 25, 47, 52, 46, 56, 55, 60],\n",
    "    'Purchase': [0, 0, 1, 1, 1, 1, 1, 1]\n",
    "}\n",
    "\n",
    "# Convert dictionary to DataFrame\n",
    "import pandas as pd\n",
    "DATA = pd.DataFrame(DATA)\n",
    "\n",
    "# Split data-\n",
    "from sklearn.model_selection import train_test_split\n",
    "X__TRAIN, X__TEST, Y__TRAIN, Y__TEST = train_test_split(DATA[[\"Age\"]],DATA[\"Purchase\"], test_size=0.3, random_state=0)\n",
    "\n",
    "# Create and train model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "LOGR = LogisticRegression()\n",
    "LOGR.fit(X__TRAIN, Y__TRAIN)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = LOGR.predict(X__TEST)\n",
    "\n",
    "\n",
    "# Evaluation Metrics\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "# Accuracy\n",
    "accuracy = accuracy_score(Y__TEST, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Precision\n",
    "precision = precision_score(Y__TEST, y_pred)\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "\n",
    "# Recall\n",
    "recall = recall_score(Y__TEST, y_pred)\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "\n",
    "# F1 Score\n",
    "f1 = f1_score(Y__TEST, y_pred)\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "\n",
    "# Calculate confusion matrix\n",
    "conf_matrix = confusion_matrix(Y__TEST, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2baccb",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## **ğŸ“Š Confusion Matrix & Goodness of Fit**\n",
    "\n",
    "The **Confusion Matrix** is a powerful tool to evaluate the performance of a classification model. It compares the actual (observed) results with predicted outcomes.\n",
    "\n",
    "### **Confusion Matrix Table:**\n",
    "\n",
    "|                              | **Observed Positive (Y) âœ…** | **Observed Negative (N) âŒ** |\n",
    "| ---------------------------- | --------------------------- | --------------------------- |\n",
    "| **Predicted Positive (Y) âœ…** | **a = TP (True Positive)**  | **b = FP (False Positive)** |\n",
    "| **Predicted Negative (N) âŒ** | **c = FN (False Negative)** | **d = TN (True Negative)**  |\n",
    "\n",
    "* **True Positives (TP)** âœ…: Correct predictions of the positive class.\n",
    "* **True Negatives (TN)** âŒ: Correct predictions of the negative class.\n",
    "* **False Positives (FP)** ğŸš«: Incorrectly predicting the positive class.\n",
    "* **False Negatives (FN)** âŒ: Incorrectly predicting the negative class.\n",
    "\n",
    "\n",
    "\n",
    "### **Goodness of Fit**\n",
    "\n",
    "The model fits well if:\n",
    "\n",
    "* **True Positives (TP)** and **True Negatives (TN)** are **high** ğŸ’¯.\n",
    "* **False Positives (FP)** and **False Negatives (FN)** are **low** ğŸ”´.\n",
    "\n",
    "\n",
    "\n",
    "### **Key Metrics:**\n",
    "\n",
    "1. **Sensitivity (Recall)**: Measures how well the model identifies positives.\n",
    "\n",
    "   $$\n",
    "   \\text{Sensitivity} = \\frac{a}{a + c}\n",
    "   $$\n",
    "\n",
    "2. **Specificity**: Measures how well the model identifies negatives.\n",
    "\n",
    "   $$\n",
    "   \\text{Specificity} = \\frac{d}{b + d}\n",
    "   $$\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ee4b22",
   "metadata": {},
   "source": [
    "| **Metric**                  | **Definition**                                                                                        | **Code**                           | **Interpretation**                                                                                                      |\n",
    "| --------------------------- | ----------------------------------------------------------------------------------------------------- | ---------------------------------- | ----------------------------------------------------------------------------------------------------------------------- |\n",
    "| **Accuracy** âœ”              | The proportion of correct predictions to total predictions.                                           | `accuracy_score(y_true, y_pred)`   | - **High Accuracy**: Model performs well overall âœ…<br> - **Low Accuracy**: Model struggles with predictions âŒ           |\n",
    "| **Precision** ğŸ¯            | The proportion of true positives to the total predicted positives.                                    | `precision_score(y_true, y_pred)`  | - **High Precision**: Few false positives, good for imbalanced data âš–ï¸<br> - **Low Precision**: More false positives ğŸ”´ |\n",
    "| **Recall (Sensitivity)** ğŸ“‰ | The proportion of true positives to the total actual positives.                                       | `recall_score(y_true, y_pred)`     | - **High Recall**: Few false negatives, good for detecting all positives ğŸ”<br> - **Low Recall**: Misses positives âš ï¸   |\n",
    "| **F1 Score** ğŸ”¥             | The harmonic mean of Precision and Recall, balancing both metrics.                                    | `f1_score(y_true, y_pred)`         | - **High F1**: Balance between Precision and Recall ğŸŒŸ<br> - **Low F1**: Imbalance in Precision and Recall ğŸ”§           |\n",
    "| **ROC AUC** ğŸ”µ              | Measures the area under the ROC curve, indicating the model's ability to distinguish between classes. | `roc_auc_score(y_true, y_pred)`    | - **High AUC**: Good at distinguishing classes ğŸ”µ<br> - **Low AUC**: Struggles to distinguish between classes âš ï¸        |\n",
    "| **Confusion Matrix** ğŸ”²     | A table showing actual vs. predicted classifications for each class.                                  | `confusion_matrix(y_true, y_pred)` | - **Diagonal elements**: Correct predictions<br> - **Off-diagonal elements**: Incorrect predictions ğŸ›‘                  |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
