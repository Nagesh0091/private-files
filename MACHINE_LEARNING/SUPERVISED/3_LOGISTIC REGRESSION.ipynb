{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6dd32a78",
   "metadata": {},
   "source": [
    "### 🎯 **Logistic Regression 🎯**\n",
    "\n",
    "### 📌 **Definition:**\n",
    "\n",
    "- Logistic Regression is used to predict a binary or categorical outcome (1 or 0, true or false) based on one or more input features.\n",
    "- Goal is to predict the probability of an observation belonging one of two categories.\n",
    "### 📈 **Equation:**\n",
    "\n",
    "**P(Y = 1) = 1 / (1 + e^-(β₀ + β₁X₁ + β₂X₂ + ... + βₙXₙ))**\n",
    "\n",
    "→ P(Y = 1) = Probability of the target being 1\n",
    "\n",
    "→ X = Features\n",
    "\n",
    "→ β = Coefficients\n",
    "\n",
    "→ e = Euler’s number (approx. 2.718)\n",
    "\n",
    "### 🧠 **Key Assumptions (VERY IMPORTANT!)**\n",
    "\n",
    "✅ **Linearity in the Logit** – Relationship between features and the log-odds of the target is linear.\n",
    "\n",
    "✅ **Independence of observations** – Each observation is independent.\n",
    "\n",
    "✅ **No multicollinearity** – Predictors should not be highly correlated.\n",
    "\n",
    "✅ **Large Sample Size** – Logistic regression generally requires a larger sample size for reliable results.\n",
    "\n",
    "### 🧮 **Types:**\n",
    "\n",
    "* **Binary Logistic Regression** → One dependent binary variable (e.g., yes/no, 0/1).\n",
    "* **Multinomial Logistic Regression** → More than two categories for the dependent variable.\n",
    "\n",
    "### 📊 **Evaluation Metrics:**\n",
    "\n",
    "* **Accuracy** – Percentage of correct predictions.\n",
    "* **Precision, Recall, F1-Score** – Metrics for imbalanced classes.\n",
    "* **AUC-ROC Curve** – Evaluates classification performance at all thresholds.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4997ed87",
   "metadata": {},
   "source": [
    "Sure! Here's the updated version with your point added:\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ **Logistic Function (Sigmoid Function)**\n",
    "\n",
    "The **logistic function**, also known as the **sigmoid function**, is a mathematical function used to model the **probability of a binary outcome**. It produces an **S-shaped curve** that maps any real-valued number to a value between **0 and 1**.\n",
    "\n",
    "### 📈 **Formula:**\n",
    "\n",
    "$$\n",
    "f(x) = \\frac{1}{1 + e^{-x}}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "* **x** = Linear combination (β₀ + β₁X₁ + β₂X₂ + ... + βₙXₙ)\n",
    "* **e** = Euler’s number (\\~2.718)\n",
    "\n",
    "### 🔍 **Output Behavior:**\n",
    "\n",
    "* **x → +∞** → **f(x) → 1**\n",
    "* **x → -∞** → **f(x) → 0**\n",
    "* **x = 0** → **f(x) = 0.5**\n",
    "\n",
    "### 📊 **Use in Logistic Regression:**\n",
    "\n",
    "* Converts the linear output into a **probability**.\n",
    "* Used to classify:\n",
    "\n",
    "  * **f(x) ≥ 0.5** → Class 1\n",
    "  * **f(x) < 0.5** → Class 0\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a4d7d33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0000\n",
      "Precision: 1.0000\n",
      "Recall: 1.0000\n",
      "F1 Score: 1.0000\n",
      "Confusion Matrix:\n",
      "[[1 0]\n",
      " [0 2]]\n"
     ]
    }
   ],
   "source": [
    "# LOGISTIC REGRESSION:\n",
    "DATA = {\n",
    "    'Age': [22, 25, 47, 52, 46, 56, 55, 60],\n",
    "    'Purchase': [0, 0, 1, 1, 1, 1, 1, 1]\n",
    "}\n",
    "\n",
    "# Convert dictionary to DataFrame\n",
    "import pandas as pd\n",
    "DATA = pd.DataFrame(DATA)\n",
    "\n",
    "# Split data-\n",
    "from sklearn.model_selection import train_test_split\n",
    "X__TRAIN, X__TEST, Y__TRAIN, Y__TEST = train_test_split(DATA[[\"Age\"]],DATA[\"Purchase\"], test_size=0.3, random_state=0)\n",
    "\n",
    "# Create and train model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "LOGR = LogisticRegression()\n",
    "LOGR.fit(X__TRAIN, Y__TRAIN)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = LOGR.predict(X__TEST)\n",
    "\n",
    "\n",
    "# Evaluation Metrics\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "# Accuracy\n",
    "accuracy = accuracy_score(Y__TEST, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Precision\n",
    "precision = precision_score(Y__TEST, y_pred)\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "\n",
    "# Recall\n",
    "recall = recall_score(Y__TEST, y_pred)\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "\n",
    "# F1 Score\n",
    "f1 = f1_score(Y__TEST, y_pred)\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "\n",
    "# Calculate confusion matrix\n",
    "conf_matrix = confusion_matrix(Y__TEST, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2baccb",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## **📊 Confusion Matrix & Goodness of Fit**\n",
    "\n",
    "The **Confusion Matrix** is a powerful tool to evaluate the performance of a classification model. It compares the actual (observed) results with predicted outcomes.\n",
    "\n",
    "### **Confusion Matrix Table:**\n",
    "\n",
    "|                              | **Observed Positive (Y) ✅** | **Observed Negative (N) ❌** |\n",
    "| ---------------------------- | --------------------------- | --------------------------- |\n",
    "| **Predicted Positive (Y) ✅** | **a = TP (True Positive)**  | **b = FP (False Positive)** |\n",
    "| **Predicted Negative (N) ❌** | **c = FN (False Negative)** | **d = TN (True Negative)**  |\n",
    "\n",
    "* **True Positives (TP)** ✅: Correct predictions of the positive class.\n",
    "* **True Negatives (TN)** ❌: Correct predictions of the negative class.\n",
    "* **False Positives (FP)** 🚫: Incorrectly predicting the positive class.\n",
    "* **False Negatives (FN)** ❌: Incorrectly predicting the negative class.\n",
    "\n",
    "\n",
    "\n",
    "### **Goodness of Fit**\n",
    "\n",
    "The model fits well if:\n",
    "\n",
    "* **True Positives (TP)** and **True Negatives (TN)** are **high** 💯.\n",
    "* **False Positives (FP)** and **False Negatives (FN)** are **low** 🔴.\n",
    "\n",
    "\n",
    "\n",
    "### **Key Metrics:**\n",
    "\n",
    "1. **Sensitivity (Recall)**: Measures how well the model identifies positives.\n",
    "\n",
    "   $$\n",
    "   \\text{Sensitivity} = \\frac{a}{a + c}\n",
    "   $$\n",
    "\n",
    "2. **Specificity**: Measures how well the model identifies negatives.\n",
    "\n",
    "   $$\n",
    "   \\text{Specificity} = \\frac{d}{b + d}\n",
    "   $$\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ee4b22",
   "metadata": {},
   "source": [
    "| **Metric**                  | **Definition**                                                                                        | **Code**                           | **Interpretation**                                                                                                      |\n",
    "| --------------------------- | ----------------------------------------------------------------------------------------------------- | ---------------------------------- | ----------------------------------------------------------------------------------------------------------------------- |\n",
    "| **Accuracy** ✔              | The proportion of correct predictions to total predictions.                                           | `accuracy_score(y_true, y_pred)`   | - **High Accuracy**: Model performs well overall ✅<br> - **Low Accuracy**: Model struggles with predictions ❌           |\n",
    "| **Precision** 🎯            | The proportion of true positives to the total predicted positives.                                    | `precision_score(y_true, y_pred)`  | - **High Precision**: Few false positives, good for imbalanced data ⚖️<br> - **Low Precision**: More false positives 🔴 |\n",
    "| **Recall (Sensitivity)** 📉 | The proportion of true positives to the total actual positives.                                       | `recall_score(y_true, y_pred)`     | - **High Recall**: Few false negatives, good for detecting all positives 🔍<br> - **Low Recall**: Misses positives ⚠️   |\n",
    "| **F1 Score** 🔥             | The harmonic mean of Precision and Recall, balancing both metrics.                                    | `f1_score(y_true, y_pred)`         | - **High F1**: Balance between Precision and Recall 🌟<br> - **Low F1**: Imbalance in Precision and Recall 🔧           |\n",
    "| **ROC AUC** 🔵              | Measures the area under the ROC curve, indicating the model's ability to distinguish between classes. | `roc_auc_score(y_true, y_pred)`    | - **High AUC**: Good at distinguishing classes 🔵<br> - **Low AUC**: Struggles to distinguish between classes ⚠️        |\n",
    "| **Confusion Matrix** 🔲     | A table showing actual vs. predicted classifications for each class.                                  | `confusion_matrix(y_true, y_pred)` | - **Diagonal elements**: Correct predictions<br> - **Off-diagonal elements**: Incorrect predictions 🛑                  |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
